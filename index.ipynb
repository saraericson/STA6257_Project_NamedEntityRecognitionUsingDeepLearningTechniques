{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"NER using RNNs and Transformer Models\"\n",
        "author: \"Sara Ericson, Andrew Campbell, Jorge Sanchez\"\n",
        "date: '`r Sys.Date()`'\n",
        "format:\n",
        "  html:\n",
        "    code-fold: true\n",
        "course: STA 6257 - Advance Statistical Modeling\n",
        "bibliography: references.bib # file contains bibtex for references\n",
        "#always_allow_html: true # this allows to get PDF with HTML features\n",
        "---"
      ],
      "id": "802d4b3b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Introduction\n",
        "\n",
        "Named Entity Recognition (NER) is a crucial task in Natural Language Processing (NLP) that involves identifying and extracting named entities from unstructured text. NER presents challenges due to the inherent complexity and ambiguity of natural language. Still, it is essential for various NLP applications, including information retrieval, question answering, and machine translation. Deep learning techniques have demonstrated exceptional performance in NER tasks in recent years. Deep learning, a subfield of machine learning, uses artificial neural networks to learn from data and make predictions.\n",
        "\n",
        "The Transformer model, introduced in 2017, has become a fundamental architecture in modern NLP and is widely used for NER tasks. It is known for its self-attention mechanism, which allows the model to weigh the importance of each word in the input sequence. Additionally, other deep learning architectures such as Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs), and Bidirectional Encoder Representations from Transformers (BERT) have been successfully applied to NER.\n",
        "\n",
        "In this literature review, we will explore and compare the effectiveness of RNNs and the Transformer model for Named Entity Recognition (NER) tasks.\n",
        "\n",
        "## 1.1 Recurrent Neural Networks (RNN)\n",
        "\n",
        "A Recurrent Neural Network (RNN) is a method of deep learning that uses sequential data or time-series data. They are often used in ordinal problems, where the output is not continues but it is discrete and ordered, and temporal problems, where data is collected over time and the order of the observations is important. These problems include speech recognition, language translation, and natural language processing (nlp). RNNs make use of training data by taking prior inputs and using them to influence current inputs and outputs. Utilizing prior inputs in this way is what creates the RNN's memory. This is something that distinguishes RNNs from other deep learning methods. RNNs are also distinguished by the sharing of parameters across each layer in the network. A popular type of RNN architecture known as Long Short-Term Memory (LSTM) has been used in named entity recognition. LSTMs have individual units in the hidden layers of a neural network, each of which has three gates. These include an input gate that controls the input information that goes into a memory cell. A forget gate that controls the amount of historical information that passes through from the previous state, and an output gate that controls the amount of information that is passed on to the next step. RNNs allow information to persist across multiple steps which enable the network to capture dependencies and context. The architecture and abilities of RNNs allow it to be a useful tool in NER that has been proven to perform better than previous NER systems.\n",
        "\n",
        "## 1.2 Transformer Model\n",
        "\n",
        "The Transformer model is a groundbreaking neural network architecture introduced in the paper \"Attention Is All You Need\" (Vaswani, 2017). â€‹The model is designed for sequence-to-sequence tasks, such as machine translation, and is known for its ability to process input sequences in parallel rather than sequentially. This parallel processing makes the Transformer model highly efficient and scalable. One of the key innovations of the Transformer model is the self-attention mechanism. Self-attention allows the model to weigh the importance of different words in the input sequence relative to each other when making predictions. The model uses multi-head attention, which means it can simultaneously attend to various input aspects. This ability to capture complex dependencies and relationships between words contributes to the model's strong performance. The Transformer architecture consists of an encoder and decoder, each composed of multiple layers of self-attention and feed forward neural networks. The encoder processes the input sequence while the decoder generates the output sequence. The connections between the encoder and decoder are facilitated by attention mechanisms that allow the decoder to focus on different parts of the input sequence as it generates the output. Given its effectiveness and efficiency in handling sequence data, the Transformer model has become the foundation for many subsequent natural language processing (NLP) models and architectures.\n",
        "\n",
        "## 1.3 Limitations\n",
        "\n",
        "***\\[Pending\\]***\n",
        "\n",
        "# **2 Methods**\n",
        "\n",
        "***\\[Pending\\]***\n",
        "\n",
        "# **3 Data Analysis and Results**\n",
        "\n",
        "## 3.1 Dataset Description\n",
        "\n",
        "I will use the dataset called corona2 from Kaggle to identify Natural Entity Recognition to identify Medical Condition, Medicine names and Pathogens. The dataset was manually tagged for training. I will use the Transformer model to apply the NER using python programming language. The dataset contains 31 observations and 4 attributes properly explained in the data definition.\n",
        "\n",
        "**Data Definition**\n",
        "\n",
        "| Column Name | Type    | Description                                         |\n",
        "|------------------|------------------|------------------------------------|\n",
        "| Text        | string  | Sentence including the labels                       |\n",
        "| Starts      | integer | Position on where the label starts                  |\n",
        "| Ends        | integer | Position on where the label ends                    |\n",
        "| Labels      | string  | The label( Medical Condition, Medicine or Pathogen) |\n",
        "\n",
        "-   Labels:\n",
        "    -   Medical condition names (example: influenza, headache, malaria)\n",
        "    -   Medicine names (example : aspirin, penicillin, ribavirin, methotrexate)\n",
        "    -   Pathogens ( example: Corona Virus, Zika Virus, cynobacteria, E. Coli)\n",
        "\n",
        "## \n",
        "\n",
        "## 3.2 Data Preparation\n",
        "\n",
        "The following python code will load the required libraries\n"
      ],
      "id": "5f203dd0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install -q nbclient\n",
        "!pip install -q requests\n",
        "!pip install -q pandas\n",
        "!pip install -q nbformat\n",
        "!pip install -q plotly.express"
      ],
      "id": "074337cd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import the data from Github to local.\n"
      ],
      "id": "3d1f53fe"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import requests\n",
        "\n",
        "# Accessing the JSON file from GitHub\n",
        "url = 'https://raw.githubusercontent.com/jsanc223/datasetCorona2/main/Corona2.json'\n",
        "# HTTP GET request to the raw URL\n",
        "response = requests.get(url)\n",
        "# I am checking if the request was successful (status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Parse the JSON data from the response\n",
        "    data = response.json()\n",
        "    print('Success, the Json data was stored into the data variable')\n",
        "else:\n",
        "    print('Failed to fetch JSON data:', response.status_code)"
      ],
      "id": "81716f3c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Parse json data into dictionary to manipulate data.\n"
      ],
      "id": "80ac0f36"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "training_data = []\n",
        "for example in data['examples']:\n",
        "  temp_dict = {}\n",
        "  temp_dict['text'] = example['content']\n",
        "  temp_dict['entities'] = []\n",
        "  for annotation in example['annotations']:\n",
        "    start = annotation['start']\n",
        "    end = annotation['end']\n",
        "    label = annotation['tag_name'].upper()\n",
        "    temp_dict['entities'].append((start, end, label))\n",
        "  training_data.append(temp_dict)"
      ],
      "id": "67abe5da",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Convert data from Dictionary to Dataframe. I am only showing the Text and Labels for each sentence\n"
      ],
      "id": "121e0caf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "# I am initialing empty lists to store the data for the DataFrame\n",
        "texts = []\n",
        "starts = []\n",
        "ends = []\n",
        "labels = []\n",
        "\n",
        "# Iterate through the training_data to extract individual entity annotations\n",
        "for example in training_data:\n",
        "    text = example['text']\n",
        "    for entity in example['entities']:\n",
        "        start, end, label = entity\n",
        "        # Append data to the lists\n",
        "        texts.append(text)\n",
        "        starts.append(start)\n",
        "        ends.append(end)\n",
        "        labels.append(label)\n",
        "\n",
        "# Create a DataFrame from the lists\n",
        "df = pd.DataFrame({'text': texts, 'start': starts, 'end': ends, 'label': labels})\n",
        "df.head(5)"
      ],
      "id": "e092dfe8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.3 Data statistics\n",
        "\n",
        "1.  Our analysis begins with examining the frequency of each label in the dataset. The dataset contains 134 instances of 'Medical Condition', 94 instances of 'Medicine', and 67 instances of 'Pathogen'. This data provides an overview of label distribution.\n"
      ],
      "id": "3776da59"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Count the occurrences of each label\n",
        "label_counts = df['label'].value_counts()\n",
        "\n",
        "# Create a DataFrame with labels and their respective counts\n",
        "df_counts = pd.DataFrame({'label': label_counts.index, 'count': label_counts.values})\n",
        "\n",
        "# Plot the frequency of each entity label using a bar plot in Plotly\n",
        "fig = px.bar(df_counts, x='label', y='count', text='count', color='label',\n",
        "             color_discrete_sequence=px.colors.qualitative.Plotly, title='Frequency of Entity Labels')\n",
        "\n",
        "# Display the counter label inside the bars\n",
        "fig.update_traces(textposition='inside')\n",
        "\n",
        "# Update axis titles\n",
        "fig.update_layout(xaxis_title='Entity Label', yaxis_title='Frequency')\n",
        "\n",
        "fig.show()"
      ],
      "id": "1d5f0489",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2.  The chart below displays the distribution of biomedical Entity labels in the dataset. 'Medical Condition' is the most prevalent at 45.4%, followed by 'Medicine' at 31.9%, and 'Pathogen' at 22.7%. The chart provides insights into the dataset composition and label prevalence..\n"
      ],
      "id": "df654254"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Get the counts of each unique label\n",
        "label_counts = df['label'].value_counts()\n",
        "# Plot a pie chart using Plotly\n",
        "fig = px.pie(label_counts, values=label_counts.values, names=label_counts.index, title='Proportion of Entity Labels', hole=0.3)\n",
        "fig.update_traces(textinfo='percent+label', textfont_size=12)\n",
        "fig.show()"
      ],
      "id": "7548ed23",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3.  The histogram below visualizes the start positions of entity labels in the text. It reveals that most entities occur within the first five hundred words.\n"
      ],
      "id": "a8f8cfd7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Plot a histogram of entity start positions using Plotly\n",
        "fig = px.histogram(df, x='start', nbins=30, title='Histogram of Entity Start Positions')\n",
        "fig.update_layout(xaxis_title='Entity Start Position', yaxis_title='Frequency')\n",
        "fig.show()"
      ],
      "id": "7b9a6fb5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.  The box plot below shows the start and end positions of entity labels, with the majority located below the five hundredth position.\n"
      ],
      "id": "8332bacb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Create box plots for 'start' and 'end' columns using Plotly\n",
        "fig = px.box(df, y=['start', 'end'], points='all', title='Box Plots of Start and End Entity Positions')\n",
        "fig.update_layout(yaxis_title='Value', xaxis_title='Column')\n",
        "fig.show()"
      ],
      "id": "fca5ee39",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **4 Transformer Model Results**\n",
        "\n",
        "***\\[Pending\\]***\n",
        "\n",
        "# **5 Recurrent Neural Networks Results **\n",
        "\n",
        "***\\[Pending\\]***\n",
        "\n",
        "# **6 Conclusion**\n",
        "\n",
        "***\\[Pending\\]***\n",
        "\n",
        "\n",
        "## Methods\n",
        "\n",
        "### Data Set\n",
        "\n",
        "https://www.kaggle.com/datasets/finalepoch/medical-ner?select=Corona2.json\n",
        "\n",
        "## Data set description\n",
        "The data was manually tagged (diseases,pathogens and medication) for training NER system item. It includes the following columns:\n",
        "\n",
        "- Text (The actual content)\n",
        "- Starts (Position on where the label starts)\n",
        "- Ends (Position on where the label ends)\n",
        "- Labels (The actual label from the text)\n",
        "- Categories:\n",
        "  - Medical condition names (example: influenza, headache, malaria)\n",
        "  - Medicine names (example : aspirin, penicillin, ribavirin, methotrexate)\n",
        "  - Pathogens ( example: Corona Virus, Zika Virus, cynobacteria, E. Coli)\n",
        "\n",
        "## Code Cell\n",
        "\n",
        "\n",
        "## Analysis and Results\n",
        "\n",
        "### Data and Vizualisation\n",
        "\n",
        "### Statistical Modeling\n",
        "\n",
        "### Conlusion\n",
        "\n",
        "## References\n",
        "\n",
        "https://arxiv.org/pdf/1910.10683.pdf\n",
        "\n",
        "https://arxiv.org/pdf/1810.04805.pdf\n",
        "\n",
        "https://arxiv.org/abs/1910.11470\n",
        "\n",
        "https://ojs.aaai.org/index.php/AAAI/article/view/3861\n",
        "\n",
        "https://arxiv.org/abs/1909.10649\n",
        "\n",
        "Devlin, J. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. ArXiv.Org. https://doi.org/10.48550/arXiv.1810.04805 \n",
        "\n",
        "Vaswani, A. (2017). Attention Is All You Need. ArXiv.Org. https://doi.org/10.48550/arXiv.1706.03762"
      ],
      "id": "48dfaf53"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}