---
title: "NER using RNNs and Transformer Models"
author: "Sara Ericson, Andrew Campbell, Jorge Sanchez"
#date: '`r Sys.Date()`'
format: revealjs
editor: visual
#course: STA 6257 - Advance Statistical Modeling
#bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
---

## NER, what is it?

::: incremental
-   Named Entity Recognition (NER) is a task in Natural Language Processing (NLP) that involves identifying and extracting named entities from unstructured text.
    -   This means identifying and classifying entities into categories that are predefined (person, place, etc.)
-   Biomedical NER is the task of text mining to specifically biomedical texts to determine entity types.
-   Similarly, medical NER tasks mine specifically medical texts for entity types.
:::

## Purpose of this review

::: incremental
-   We are testing different deep learning techniques and their effectiveness of NER on data containing information on Medical Conditions, Medicine Names, and Pathogens.

-   The methods we chose are:

    -   Recurrent Neural Network (RNN): a method of deep learning that is used for sequential data and time-series data.
    -   Transformer Model: a model designed for sequence-to-sequence tasks, such as machine translation, and is known for its ability to process input sequences in parallel rather than sequentially.
:::

## Limitations

::: incremental
-   Current NER techniques have major limitations being that the standard language models are unidirectional, operating in a single direction, and thus limit the choice of architecture that can be used for pre-training.
-   While the deep learning techniques we have chosen are not perfect, they do directly address this major limitation and we will show their effectiveness moving forward.
-   Limitations of our chosen techniques:
    -   Transformer Model: The size of the training data can be a limitation here. If there is not enough training data it can make the model less effective.
    -   RNN: RNN's are prone to overfitting which is especially true when there is a small dataset.
:::

## What is Transformer Model?

::: incremental
-   The Transformer model is a neural network architecture designed for sequence-to-sequence tasks such as machine translation.

-   The model's key innovation is the self-attention mechanism, which allows it to weight the importance of different words in the input sequence.

-   Self-attention enables the model to capture long-range dependencies and handle complex language structures effectively.
:::

## Transformer Architecture

::: columns
::: {.column width="60%"}
![](images/transformer%20Architecture.png)
:::

::: {.column width="40%"}
-   Encoder

-   Decoder

-   Positional Encoding

-   Multi-Head Attention
:::
:::

## Transformer Methods

::: incremental
-   Scaled Dot-Product Attention formula\
    \
    $\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V$\

-   The Single (Masked) Self- or Cross-Attention Head Formula $\begin{align*}\text{Attention}(Q, K, V) &= \text{softmax}\left(\frac{QK^\top + \text{Mask}}{\sqrt{d_k}}\right)V \\\end{align*}$
:::

## Transformer - How to do it?

::: incrememtal
1.  Annotated Dataset for Training

    -   Labelstud.io , Prodi.gy

2.  Pre-process the dataset

    -   Tokenization - NLTK

3.  Fine-tune a pre-trained Tranformer Model

    -   Huggingface - BERT

4.  Train

5.  Evaluate

    -   Metrics: Precision,recall and F1 score.
:::

## Transformer Limitations

::: incremental
-   Standard language models are unidirectional, restricting pre-training architecture options and limiting context awareness.

-   Transformers have high computational complexity due to numerous parameters, requiring significant resources and specialized hardware for deep models and long sequences.
:::

## Dataset

::: incremental
We will use the dataset called corona2 from Kaggle to identify Natural Entity Recognition to identify Medical Condition, Medicine names and Pathogens. Similarly to the dataset used in the dataset was manually tagged for training.

-   Labels:

    -   Medical condition names (example: influenza, headache, malaria)
    -   Medicine names (example : aspirin, penicillin, ribavirin, methotrexate)
    -   Pathogens ( example: Corona Virus, Zika Virus, cynobacteria, E. Coli)
:::

## Dataset Definition

::: incremental
| Column Name | Type    | Description                                         |
|-------------------|-------------------|----------------------------------|
| Text        | string  | Sentence including the labels                       |
| Starts      | integer | Position on where the label starts                  |
| Ends        | integer | Position on where the label ends                    |
| Labels      | string  | The label( Medical Condition, Medicine or Pathogen) |
:::

## Dataset Sample

::: incremental
Text: Buprenorphine has been shown experimentally (1982--1995) to be effective against severe, refractory depression.

::: columns
::: {.column width="50%"}
![](images/annotated1.png)
:::

::: {.column width="50%"}
![](images/annotated2.png)
:::
:::
:::

## Dataset Visualization - Labels

::: incremental
![](images/frequency_labels-01.png){fig-align="center"}
:::

## Data Visualization - Position

::: incremental
![](images/start_end_Positions-01.png){fig-align="center"}
:::

## What is a Recurrent Neural Network (RNN) starting with a Neural Network

::: incremental
-   A basic neural network has an input layer, a hidden layer or layers, and an output layer
-   Between the layers are weights and biases that are used in the calculation of the output
-   The network is optimized using back propagation and a gradient descent algorithm
-   Back propagation is used to find the gradient of a parameter
-   A gradient descent algorithm is used to find values that minimize a loss function (such as error)

Example of a Loss function (Mean Squared Error)\
$\text{L(ùúΩ)} = (1/N) * ‚àë(y_i - y_i^*)^2$\
\
Gradient Descent Algorithm\
$\text{ùúΩ}_j =ùúΩ_j - ùõº (‚àÇJ(ùúΩ) / ‚àÇùúΩ_j)$\

N : number of vector entries with yi in output vector

yi : predicted value

yi\* : actual value

ùõº : learning rate

ùúΩj : input
:::

## What make a RNN different

::: incremental
-   A recurrent neural network is similar to a basic neural network but with the addition of a feedback loop

-   In these networks activation functions (∆í) are used to determine whether a neuron in the network is turned on or off

-   The data in feedback loop is included with the input of subsequent data

-   This allows for past data to influence the calculations of future outputs which is why RNNs are great for sequential data such as text

    ![](images/Screen%20Shot%202023-04-26%20at%206.14.17%20PM.png){width="562"}
:::

## Limitations of RNN

::: incremental
-   One of the main problems RNNs face is an exploding or vanishing gradient

-   This can occur when a large amount of data is used

-   The weight in the feedback loop (W_h) becomes multiplied on itself over and over which will results in an extremely large or extremely small value

-   For example if the weight is 3 and there are 10 data points the weight will end up being 3 to the power of then (59049). When applied to larger data sets this number can become much larger. This number is then used in the gradient descent calculation resulting in an optimization step that is way too big

-   For smaller weights such as 0.5 with 10 data points the weight will be 0.000977. This will result in an optimization step that is way too small
:::

## Solution to exploding and vanishing gradient

::: incremental
-   A common solution to exploding and vanishing gradient is a Long Short-Term Memory (LSTM) cell

-   These cells are located in the feedback loop and contain three gates within the cell

    1.  Input gate: determines which information is stored in the cell

    2.  Forget gate: determines which information will be discarded

    3.  Output gate: provides the activation for the final output

-   These gates use a sigmoid function which outputs a value between zero and one

-   Zero blocks all information and one allows all information through

```{=html}
<!-- -->
```
    Sigmoid:\
    $\text{f(x)} = {1/(1+e^{-x})}$\

    General gate equation:\
    $\text{g}_i = ùùà(w[h_{i-1}, x_i] + b)$\

    x : input

    ùùà : sigmoid function

    w : weight

    h : information of i-th iteration

    b : bias
:::

## RNN Continued

::: incremental
-   LSTMs maintain a cell state as a result of the calculations from the equations below along with the sigmoid and the general gate equation

-   This state is what provides the "memory" which makes it useful in named entity recognition

    Possible cell state at i-th iteration:\
    $\text{c}_i^*=tanh(w[h_{i-1}, x_i] + b)$\
    \
    Cell state:\
    $\text{c}_i = (forget gate * c_{i-1}) + (input gate * c_i^* )$\
    \
    Output:\
    $\text{h}_i = (output gate * tanh(c_i))$\
    \
    tanh:\
    $\text{f(x)} = (e^x - e^{-x})/(e^x + e^{-x})$\

-   Using LSTM cells in recurrent neural networks allows the network to be applied to large datasets which is very beneficial in the context of named entity recognition
:::

\

# 3. Data Analysis and Results

## 3.1 Dataset Description

We will use the dataset called corona2 from Kaggle to identify Natural Entity Recognition to identify Medical Condition, Medicine names and Pathogens. Similarly to the dataset used in [@xiong2021Improving], the dataset was manually tagged for training. We will use the Transformer model and the RNN model to apply the NER using python programming language. The dataset contains 31 observations and 4 attributes properly explained in the data definition.

## Data Definition

| Column Name | Type    | Description                                         |
|-------------------|-------------------|----------------------------------|
| Text        | string  | Sentence including the labels                       |
| Starts      | integer | Position on where the label starts                  |
| Ends        | integer | Position on where the label ends                    |
| Labels      | string  | The label( Medical Condition, Medicine or Pathogen) |

## Data Labels

-   Labels:
    -   Medical condition names (example: influenza, headache, malaria)
    -   Medicine names (example : aspirin, penicillin, ribavirin, methotrexate)
    -   Pathogens ( example: Corona Virus, Zika Virus, cynobacteria, E. Coli)

## 3.2 Data Preparation

The following python code will load the required libraries:

-   **nbclient** : Executes Jupyter notebooks programmatically

-   **requests** : Sends HTTP requests and interacts with RESTful APIs in Python

-   **pandas** : Manipulates and analyzes tabular data using DataFrame and Series

-   **nbformat** : Reads, writes, and manipulates Jupyter Notebook files

-   **plotly.express** : Creates interactive data visualizations with a simple interface

## Code for analysis

``` r
library(reticulate)
# do the following ONCE AND COMMENT
py_install("nbclient")
py_install("requests")
py_install("pandas")
py_install("nbformat")
py_install("plotly")
```

```{r}
library(reticulate)
# do the following ONCE AND COMMENT
py_install("nbclient")
py_install("requests")
py_install("pandas")
py_install("nbformat")
py_install("plotly")
```

``` python
#| echo: false
import pandas
import nbclient
import requests
import nbformat
import plotly.express
```

```{python}
#| echo: false
import pandas
import nbclient
import requests
import nbformat
import plotly.express
```

## Import the data from Github to local.

``` python
url = 'https://raw.githubusercontent.com/jsanc223/datasetCorona2/main/Corona2.json'
# HTTP GET request to the raw URL
response = requests.get(url)
# I am checking if the request was successful (status code 200)
if response.status_code == 200:
    # Parse the JSON data from the response
    data = response.json()
    print('Success, the Json data was stored into the data variable')
else:
    print('Failed to fetch JSON data:', response.status_code)
```

```{python}
url = 'https://raw.githubusercontent.com/jsanc223/datasetCorona2/main/Corona2.json'
# HTTP GET request to the raw URL
response = requests.get(url)
# I am checking if the request was successful (status code 200)
if response.status_code == 200:
    # Parse the JSON data from the response
    data = response.json()
    print('Success, the Json data was stored into the data variable')
else:
    print('Failed to fetch JSON data:', response.status_code)
```

## Parse json data into dictionary to manipulate data.

``` python
training_data = []
for example in data['examples']:
  temp_dict = {}
  temp_dict['text'] = example['content']
  temp_dict['entities'] = []
  for annotation in example['annotations']:
    start = annotation['start']
    end = annotation['end']
    label = annotation['tag_name'].upper()
    temp_dict['entities'].append((start, end, label))
  training_data.append(temp_dict)
```

```{python}
training_data = []
for example in data['examples']:
  temp_dict = {}
  temp_dict['text'] = example['content']
  temp_dict['entities'] = []
  for annotation in example['annotations']:
    start = annotation['start']
    end = annotation['end']
    label = annotation['tag_name'].upper()
    temp_dict['entities'].append((start, end, label))
  training_data.append(temp_dict)
```

## Convert data from Dictionary to Dataframe

``` python
import pandas as pd
# I am initialing empty lists to store the data for the DataFrame
texts = []
starts = []
ends = []
labels = []

# Iterate through the training_data to extract individual entity annotations
for example in training_data:
    text = example['text']
    for entity in example['entities']:
        start, end, label = entity
        # Append data to the lists
        texts.append(text)
        starts.append(start)
        ends.append(end)
        labels.append(label)

# Create a DataFrame from the lists
df = pd.DataFrame({'text': texts, 'start': starts, 'end': ends, 'label': labels})
df.head(5)
```

## Data as dataframe

```{python}
import pandas as pd
# I am initialing empty lists to store the data for the DataFrame
texts = []
starts = []
ends = []
labels = []

# Iterate through the training_data to extract individual entity annotations
for example in training_data:
    text = example['text']
    for entity in example['entities']:
        start, end, label = entity
        # Append data to the lists
        texts.append(text)
        starts.append(start)
        ends.append(end)
        labels.append(label)

# Create a DataFrame from the lists
df = pd.DataFrame({'text': texts, 'start': starts, 'end': ends, 'label': labels})
df.head(5)
```

## 3.3 Data statistics

1.  The dataset contains 134 instances of 'Medical Condition', 94 instances of 'Medicine', and 67 instances of 'Pathogen'.

``` python
import plotly.express as px

# Count the occurrences of each label
label_counts = df['label'].value_counts()

# Create a DataFrame with labels and their respective counts
df_counts = pd.DataFrame({'label': label_counts.index, 'count': label_counts.values})

# Plot the frequency of each entity label using a bar plot in Plotly
fig = px.bar(df_counts, x='label', y='count', text='count', color='label',
             color_discrete_sequence=px.colors.qualitative.Plotly, title='Frequency of Entity Labels')

fig.update_layout(xaxis_title='Entity Label', yaxis_title='Frequency')

# Display the counter label inside the bars
#fig.update_traces(textposition='inside')
# Update axis titles
#fig.update_layout(xaxis_title='Entity Label', yaxis_title='Frequency')

#fig.show()
```

## This data provides an overview of label distribution.

```{python}
import plotly.express as px

# Count the occurrences of each label
label_counts = df['label'].value_counts()

# Create a DataFrame with labels and their respective counts
df_counts = pd.DataFrame({'label': label_counts.index, 'count': label_counts.values})

# Plot the frequency of each entity label using a bar plot in Plotly
fig = px.bar(df_counts, x='label', y='count', text='count', color='label',
             color_discrete_sequence=px.colors.qualitative.Plotly, title='Frequency of Entity Labels')

fig.update_layout(xaxis_title='Entity Label', yaxis_title='Frequency')

# Display the counter label inside the bars
#fig.update_traces(textposition='inside')
# Update axis titles
#fig.update_layout(xaxis_title='Entity Label', yaxis_title='Frequency')

#fig.show()
```

## Data Statistics Continued

2.  The chart below displays the distribution of biomedical Entity labels in the dataset. 'Medical Condition' is the most prevalent at 45.4%, followed by 'Medicine' at 31.9%, and 'Pathogen' at 22.7%. The chart provides insights into the dataset composition and label prevalence..

``` python

import plotly.express as px

# Get the counts of each unique label
label_counts = df['label'].value_counts()
# Plot a pie chart using Plotly
fig = px.pie(label_counts, values=label_counts.values, names=label_counts.index, title='Proportion of Entity Labels', hole=0.3)
fig.update_traces(textinfo='percent+label', textfont_size=12)
#fig.show()
```

## Chart of Date

```{python}

import plotly.express as px

# Get the counts of each unique label
label_counts = df['label'].value_counts()
# Plot a pie chart using Plotly
fig = px.pie(label_counts, values=label_counts.values, names=label_counts.index, title='Proportion of Entity Labels', hole=0.3)
fig.update_traces(textinfo='percent+label', textfont_size=12)
#fig.show()

```

## Data Statistics Continued

3.  The histogram below visualizes the start positions of entity labels in the text. It reveals that most entities occur within the first five hundred words.

``` python

import plotly.express as px

# Plot a histogram of entity start positions using Plotly
fig = px.histogram(df, x='start', nbins=30, title='Histogram of Entity Start Positions')
fig.update_layout(xaxis_title='Entity Start Position', yaxis_title='Frequency')
#fig.show()
```

## Histogram of Date

```{python}

import plotly.express as px

# Plot a histogram of entity start positions using Plotly
fig = px.histogram(df, x='start', nbins=30, title='Histogram of Entity Start Positions')
fig.update_layout(xaxis_title='Entity Start Position', yaxis_title='Frequency')
#fig.show()
```

## Data Statistics Continued

4.  The box plot below shows the start and end positions of entity labels, with the majority located below the five hundredth position.

``` python

import plotly.express as px

# Create box plots for 'start' and 'end' columns using Plotly
fig = px.box(df, y=['start', 'end'], points='all', title='Box Plots of Start and End Entity Positions')
fig.update_layout(yaxis_title='Value', xaxis_title='Column')
#fig.show()
```

## Boxplot of Data

```{python}

import plotly.express as px

# Create box plots for 'start' and 'end' columns using Plotly
fig = px.box(df, y=['start', 'end'], points='all', title='Box Plots of Start and End Entity Positions')
fig.update_layout(yaxis_title='Value', xaxis_title='Column')
#fig.show()

```

# Results

## Transformer Model Code

``` python
training_data = []
for example in data['examples']:
  temp_dict = {}
  temp_dict['text'] = example['content']
  temp_dict['entities'] = []
  for annotation in example['annotations']:
    start = annotation['start']
    end = annotation['end']
    label = annotation['tag_name'].upper()
    temp_dict['entities'].append((start, end, label))
  training_data.append(temp_dict)
print('Our dataset contains the following number of samples: ', len(training_data))
```

```{python}
training_data = []
for example in data['examples']:
  temp_dict = {}
  temp_dict['text'] = example['content']
  temp_dict['entities'] = []
  for annotation in example['annotations']:
    start = annotation['start']
    end = annotation['end']
    label = annotation['tag_name'].upper()
    temp_dict['entities'].append((start, end, label))
  training_data.append(temp_dict)
print('Our dataset contains the following number of samples: ', len(training_data))
```

## Transformer Model Code Continued

This code formats entity annotations for our NER task. It selects the first training example, retrieves the text and annotated entities, and presents them in a table with start and end positions and labels. The output includes the text and a table of entities.

``` python
import pandas as pd
example = training_data[0]
# Extract text and entities from the selected example
text = example['text']
entities = example['entities']
# Create a dataframe to store the entities with column names: Start, End, Label
entities_df = pd.DataFrame(entities, columns=['Start', 'End', 'Label'])
# Display the text and entities table
print("Text:", text)
print("Entities:")
print(entities_df)
```

## Printed Output

```{python}
import pandas as pd
example = training_data[0]

# Extract text and entities from the selected example
text = example['text']
entities = example['entities']

# Create a dataframe to store the entities with column names: Start, End, Label
entities_df = pd.DataFrame(entities, columns=['Start', 'End', 'Label'])

# Display the text and entities table
print("Text:", text)
print("Entities:")
print(entities_df)
```

## Transformer Model Code Continued

``` python
import pandas as pd
# Initialize empty lists to store the data for the DataFrame
texts = []
starts = []
ends = []
labels = []

# Iterate through the training_data to extract individual entity annotations
for example in training_data:
    text = example['text']
    for entity in example['entities']:
        start, end, label = entity
        # Append data to the lists
        texts.append(text)
        starts.append(start)
        ends.append(end)
        labels.append(label)

# Create a DataFrame from the lists
df = pd.DataFrame({'text': texts, 'start': starts, 'end': ends, 'label': labels})
df
```

## Executed Code Results

```{python}
import pandas as pd
# Initialize empty lists to store the data for the DataFrame
texts = []
starts = []
ends = []
labels = []

# Iterate through the training_data to extract individual entity annotations
for example in training_data:
    text = example['text']
    for entity in example['entities']:
        start, end, label = entity
        # Append data to the lists
        texts.append(text)
        starts.append(start)
        ends.append(end)
        labels.append(label)

# Create a DataFrame from the lists
df = pd.DataFrame({'text': texts, 'start': starts, 'end': ends, 'label': labels})
df
```

## Transformer Model Results Continued

The Transformer model needed to be deployed in a Google Colab Repo because our computers did not have the power to train the transformer model when running out of a CPU. We get the following results which are good besides the eval_loss with over 31%. The hyperparameters were modified by increasing the epochs from 3 to 5 and 5 to 10, but the loss decreased to 25%. Precision and accuracy did not improve. The best approach to reduce the loss is to get more data. We could not find the more annotated dataset for NER in health. In Google Colab, a Hardware accelerator (GPU) was used to increase the training speed, and it worked. The link for further review is provided [Google Colab Code](https://colab.research.google.com/drive/1qKpXd2sP-Iesi76W-232hVUPkl9zvl5s?usp=sharing)

{'eval_loss': 0.3150147497653961, 'eval_precision': 0.9375, 'eval_recall': 1.0, 'eval_f1': 0.967741935483871, 'eval_accuracy': 0.9375, 'eval_runtime': 9.0895, 'eval_samples_per_second': 0.77, 'eval_steps_per_second': 0.11, 'epoch': 3.0}

## 5 RNN Results

Load required libraries for the RNN.

``` python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import LSTM, Dense, Dropout, TimeDistributed, Bidirectional
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
```

```{python}
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import LSTM, Dense, Dropout, TimeDistributed, Bidirectional
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
```

## RNN Results Continued

Using the dataframe created from the transformer model a vocabulary of words and labels was created and then converted into indices. The sequences are then padded to a maximum length and then converted to one-hot encoded vectors. A one-hot encoded vector is a binary vector in which the label is encoded as 1 and everything else that is not the label is encoded as 0. This is necessary to train the model with tensorflow and recurrent neural networks.

## RNN Code

``` python
words = set(df['text'].values)
word2idx = {w: i + 2 for i, w in enumerate(words)}
word2idx['PAD'] = 0
word2idx['UNK'] = 1

tags = set(df['label'].values)
tag2idx = {t: i + 1 for i, t in enumerate(tags)}
tag2idx['PAD'] = 0

X = [[word2idx.get(w, 1) for w in sentence.split()] for sentence in df['text'].values]
y = [[tag2idx[t] for t in sentence.split()] for sentence in df['label'].values]

maxlen = max(len(x) for x in X)
X = pad_sequences(X, padding='post', maxlen=maxlen)
y = pad_sequences(y, padding='post', maxlen=maxlen)
y = to_categorical(y, num_classes=len(tag2idx))
```

```{python}
words = set(df['text'].values)
word2idx = {w: i + 2 for i, w in enumerate(words)}
word2idx['PAD'] = 0
word2idx['UNK'] = 1

tags = set(df['label'].values)
tag2idx = {t: i + 1 for i, t in enumerate(tags)}
tag2idx['PAD'] = 0

X = [[word2idx.get(w, 1) for w in sentence.split()] for sentence in df['text'].values]
y = [[tag2idx[t] for t in sentence.split()] for sentence in df['label'].values]

maxlen = max(len(x) for x in X)
X = pad_sequences(X, padding='post', maxlen=maxlen)
y = pad_sequences(y, padding='post', maxlen=maxlen)
y = to_categorical(y, num_classes=len(tag2idx))
```

## After the data is manipulated the RNN model is created and compiled with the use of tensorflow

```{python}
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(len(word2idx), 128),
    tf.keras.layers.LSTM(128, return_sequences=True),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(len(tag2idx), activation='softmax'))
])
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
```

``` python
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(len(word2idx), 128),
    tf.keras.layers.LSTM(128, return_sequences=True),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(len(tag2idx), activation='softmax'))
])
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
```

## Lastly the model is fitted and the accuracy is calculated

The RNN model is shown to be very effective with an accuracy of 0.9967.

``` python
model.fit(X, y, batch_size=32, epochs=10)
```

```{python}
model.fit(X, y, batch_size=32, epochs=10, verbose=2)
```

## Conclusion

::: incremental
-   NER is a key NLP task that involves identifying named entities in a text.
-   Deep learning techniques have become more mainstream in the NER field due to their major advantages.
-   The techniques we have chose, RNN and Transformer Model, have shown this to be true for various situations.
-   The accuracies of our models were:
    -   Transformer Model: 0.9375
    -   RNN: 0.9967
-   This shows that when doing NER for our chosen dataset, RNN was more effective after training, but that both are over 90% accurate given the data that we have used.
:::

# References

::: incremental
[@raffel2020exploring] [@souza2020portuguese]
:::
