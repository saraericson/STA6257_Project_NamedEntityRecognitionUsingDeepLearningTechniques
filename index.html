<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.189">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Sara Ericson, Andrew Campbell, Jorge Sanchez">
<meta name="dcterms.date" content="2023-04-09">

<title>Named Entity Recognition using Deep Learning Techniques</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="index_files/libs/clipboard/clipboard.min.js"></script>
<script src="index_files/libs/quarto-html/quarto.js"></script>
<script src="index_files/libs/quarto-html/popper.min.js"></script>
<script src="index_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="index_files/libs/quarto-html/anchor.min.js"></script>
<link href="index_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="index_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="index_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="index_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Named Entity Recognition using Deep Learning Techniques</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Sara Ericson, Andrew Campbell, Jorge Sanchez </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 9, 2023</p>
    </div>
  </div>
    
  </div>
  

</header>

<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Named Entity Recognition (NER) is a critical component in Natural Language Processing (NLP) that aims to identify and extract named entities from unstructured text. It is a challenging task due to the complexity and ambiguity of natural language, and it plays a vital role in various NLP applications such as information retrieval, question answering, and machine translation. In recent years, deep learning techniques have shown great promise in achieving state-of-the-art results in NER tasks. Deep learning is a subset of machine learning that involves training artificial neural networks to learn from data and make predictions. One of the most popular deep learning architectures for NER is the Transformer model, which was introduced in 2017 and has since become a cornerstone of modern NLP. In addition to the Transformer model, there are many other deep learning techniques that have been applied to NER, including Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs), and Bidirectional Encoder Representations from Transformers (BERT), among others.</p>
<p>NER is very useful in biomedical text mining. Previous NER methods for biomedical text mining rely on dictionary or rule based methods and machine learning techniques which are time consuming and have been proven to perform worse than deep learning methods. However, accurate deep neural network systems for NER have only recently been developed. Complications of NER occur due to the category of named entities being dependent on the context of the surrounding text and named entities having multiple definitions and evaluation criteria. Neural network (NN) systems are preferential to other machine learning systems because they require less feature engineering, so they are more domain independent. Evaluation of NER performance can be done in different ways. It can be based on type, which is whether the predicted label was correct regardless of entity boundaries, or on text, which is whether the predicted label was correct regardless of label. The precision is the correct predictions divided by the total number of predictions. Recall is the number of entities a system predicted correctly divided by the number that were identified by human annotators. A statistic that is often used when evaluating NER performance is the F-Score which is the harmonic mean of precision and recall from both type and text.</p>
<p>In this research project, we explore various deep learning techniques for NER and compare their performance on a benchmark dataset, with the goal of improving the accuracy and efficiency of NER systems.</p>
<p>We are utilizing the articles and summaries under the references heading. This is a working introduction and the articles are linked below.</p>
</section>
<section id="data-set" class="level2">
<h2 class="anchored" data-anchor-id="data-set">Data Set</h2>
<p>https://www.kaggle.com/datasets/finalepoch/medical-ner?select=Corona2.json</p>
<section id="recurrent-neural-network-rnn" class="level3">
<h3 class="anchored" data-anchor-id="recurrent-neural-network-rnn">Recurrent Neural Network (RNN)</h3>
<p>A Recurrent Neural Network (RNN) is a method of deep learning that uses sequential data or time-series data. They are often used in ordinal problems, where the output is not continues but it is discrete and ordered, and temporal problems, where data is collected over time and the order of the observations is important. These problems include speech recognition, language translation, and natural language processing (nlp). RNNs make use of training data by taking prior inputs and using them to influence current inputs and outputs. Utilizing prior inputs in this way is what creates the RNN’s memory. This is something that distinguishes RNNs from other deep learning methods. RNNs are also distinguished by the sharing of parameters across each layer in the network. A popular type of RNN architecture known as Long Short-Term Memory (LSTM) has been used in named entity recognition. LSTMs have individual units in the hidden layers of a neural network, each of which has three gates. These include an input gate that controls the input information that goes into a memory cell. A forget gate that controls the amount of historical information that passes through from the previous state, and an output gate that controls the amount of information that is passed on to the next step. RNNs allow information to persist across multiple steps which enable the network to capture dependencies and context. The architecture and abilities of RNNs allow it to be a useful tool in NER that has been proven to perform better than previous NER systems</p>
</section>
</section>
<section id="transformers-and-bert-a-breif-overview" class="level2">
<h2 class="anchored" data-anchor-id="transformers-and-bert-a-breif-overview">Transformers and BERT: A Breif Overview</h2>
<section id="transformers" class="level3">
<h3 class="anchored" data-anchor-id="transformers">Transformers</h3>
<p>The Transformer model is a groundbreaking neural network architecture introduced in the paper “Attention Is All You Need” (Vaswani, 2017). The model is designed for sequence-to-sequence tasks, such as machine translation, and is known for its ability to process input sequences in parallel rather than sequentially. This parallel processing makes the Transformer model highly efficient and scalable. One of the key innovations of the Transformer model is the self-attention mechanism. Self-attention allows the model to weigh the importance of different words in the input sequence relative to each other when making predictions. The model uses multi-head attention, which means it can simultaneously attend to various input aspects. This ability to capture complex dependencies and relationships between words contributes to the model’s strong performance. The Transformer architecture consists of an encoder and decoder, each composed of multiple layers of self-attention and feedforward neural networks. The encoder processes the input sequence while the decoder generates the output sequence. The connections between the encoder and decoder are facilitated by attention mechanisms that allow the decoder to focus on different parts of the input sequence as it generates the output. Given its effectiveness and efficiency in handling sequence data, the Transformer model has become the foundation for many subsequent natural language processing (NLP) models and architectures. The architecture of this model consists of the following key components 1. The Transformer model is structured into two core segments: an encoder responsible for interpreting and encoding the input sequence and a decoder that builds the final output based on the encoder’s representation. Both the encoder and decoder are composed of a series of identical layers stacked on each other. 2. Each layer in the encoder has two key components: multi-head self-attention, which allows the model to weigh the relevance of different input elements, and a feedforward neural network, which is applied to each position separately. 3. Decoder layers also have three key components: multi-head self-attention, similar to the encoder; multi-head cross-attention, which pays attention to the encoder’s output; and a feedforward neural network, like the one in the encoder. 4. Multi-head attention is a feature that lets the model focus on different aspects of the input data with multiple “attention heads.” This mechanism helps the model understand input data more effectively. 5. Positional encoding gives the model information about the order of words in a sequence. It’s added to the initial word embeddings and helps the model understand the position of each word. 6. Once the decoder completes its processing, the resulting output is directed through two subsequent layers: a linear layer and a SoftMax layer. These layers work together to generate a probability distribution across the entire target vocabulary. From this distribution, the model selects the word with the highest probability as the final output for each position in the sequence.</p>
<p><strong><em>INSERT PHOTO!!!</em></strong></p>
<p>The Transformer model is a robust neural network for handling sequential data. It uses attention mechanisms to understand the relationships between different elements in the input and produces high-quality output for tasks like language translation.</p>
</section>
<section id="bert-bidirectional-encoder-representations-from-transformers" class="level3">
<h3 class="anchored" data-anchor-id="bert-bidirectional-encoder-representations-from-transformers">BERT (Bidirectional Encoder Representations from Transformers)</h3>
<p>BERT, which stands for Bidirectional Encoder Representations from Transformers, is a powerful pre-trained language model introduced in the paper “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding” (Devlin, 2018). BERT has achieved state-of-the-art results on various NLP tasks, including text classification, named entity recognition, and question answering. One of the distinguishing features of BERT is its ability to capture bidirectional context. Unlike traditional language models that process text from left to right or right to left, BERT considers both the left and right context when making predictions. This bidirectional context is achieved through a pre-training objective called masked language modeling (MLM). In MLM, specific tokens in the input text are randomly masked, and the model is trained to predict the masked tokens based on the surrounding context. BERT is also pre-trained using a next-sentence prediction (NSP) objective, which trains the model to predict whether two sentences follow each other in the original text. This objective helps BERT understand the relationships between sentences. BERT’s pre-training phase is conducted on large unannotated text corpora, resulting in a language model that captures rich language representations. The pre-trained BERT model is then fine-tuned on specific NLP tasks using relatively small amounts of labeled data, resulting in a strong performance across diverse NLP benchmarks. BERT’s success has led to the development of numerous variants and adaptations of the model, and it has become one of the most influential models in NLP research and applications. The following figure presents examples of both pre-training tasks using a pair of input sentences. It also highlights how the BERT model transforms the input into token sequences for processing. The critical components are as follows: 1. Input Representation: BERT’s input representation includes three key components: token embeddings, segment embeddings, and position embeddings. These embeddings are combined to create a comprehensive representation of each token. The input sequence starts with the [CLS] token (classification token) and uses the [SEP] token (separator token) to separate the sentences. 2. Masked Language Model (MLM): The MLM task involves randomly masking specific tokens in the input sequence and training the model to predict the masked tokens based on the context provided by the unmasked tokens. In the figure, the word “making” is masked and replaced with the [MASK] token, and the model predicts the original word. 3. Next Sentence Prediction (NSP): The NSP task trains the model to understand the relationships between pairs of sentences. The model predicts whether the second sentence will likely follow the first sentence. This task is binary, with the possible predictions being “IsNext” (the second sentence follows the first) or “NotNext” (the second sentence is random). In the figure, the model predicts “IsNext.”</p>
<p><strong><em>INSERT PHOTO!!!</em></strong></p>
<p>Overall, this figure provides a visual depiction of the input representation used by BERT and the two pre-training tasks that contribute to its language understanding capabilities. The MLM task helps BERT understand language context, while the NSP task allows it to understand sentence-level relationships. These pre-training tasks enable BERT to learn deep bidirectional representations, making it a powerful language model.</p>
</section>
</section>
<section id="methods" class="level2">
<h2 class="anchored" data-anchor-id="methods">Methods</h2>
<p>The common non-parametric regression model is <span class="math inline">\(Y_i = m(X_i) + \varepsilon_i\)</span>, where <span class="math inline">\(Y_i\)</span> can be defined as the sum of the regression function value <span class="math inline">\(m(x)\)</span> for <span class="math inline">\(X_i\)</span>. Here <span class="math inline">\(m(x)\)</span> is unknown and <span class="math inline">\(\varepsilon_i\)</span> some errors. With the help of this definition, we can create the estimation for local averaging i.e.&nbsp;<span class="math inline">\(m(x)\)</span> can be estimated with the product of <span class="math inline">\(Y_i\)</span> average and <span class="math inline">\(X_i\)</span> is near to <span class="math inline">\(x\)</span>. In other words, this means that we are discovering the line through the data points with the help of surrounding data points. The estimation formula is printed below <span class="citation" data-cites="R-base">(<a href="#ref-R-base" role="doc-biblioref">R Core Team 2019</a>)</span>:</p>
<p><span class="math display">\[
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
\]</span> <span class="math inline">\(W_n(x)\)</span> is the sum of weights that belongs to all real numbers. Weights are positive numbers and small if <span class="math inline">\(X_i\)</span> is far from <span class="math inline">\(x\)</span>.</p>
</section>
<section id="analysis-and-results" class="level2">
<h2 class="anchored" data-anchor-id="analysis-and-results">Analysis and Results</h2>
<section id="data-and-vizualisation" class="level3">
<h3 class="anchored" data-anchor-id="data-and-vizualisation">Data and Vizualisation</h3>
<p>A study was conducted to determine how…</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># loading packages </span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(knitr)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggthemes)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggrepel)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dslabs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load Data</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kable</span>(<span class="fu">head</span>(murders))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<table class="table table-sm table-striped">
<thead>
<tr class="header">
<th style="text-align: left;">state</th>
<th style="text-align: left;">abb</th>
<th style="text-align: left;">region</th>
<th style="text-align: right;">population</th>
<th style="text-align: right;">total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Alabama</td>
<td style="text-align: left;">AL</td>
<td style="text-align: left;">South</td>
<td style="text-align: right;">4779736</td>
<td style="text-align: right;">135</td>
</tr>
<tr class="even">
<td style="text-align: left;">Alaska</td>
<td style="text-align: left;">AK</td>
<td style="text-align: left;">West</td>
<td style="text-align: right;">710231</td>
<td style="text-align: right;">19</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Arizona</td>
<td style="text-align: left;">AZ</td>
<td style="text-align: left;">West</td>
<td style="text-align: right;">6392017</td>
<td style="text-align: right;">232</td>
</tr>
<tr class="even">
<td style="text-align: left;">Arkansas</td>
<td style="text-align: left;">AR</td>
<td style="text-align: left;">South</td>
<td style="text-align: right;">2915918</td>
<td style="text-align: right;">93</td>
</tr>
<tr class="odd">
<td style="text-align: left;">California</td>
<td style="text-align: left;">CA</td>
<td style="text-align: left;">West</td>
<td style="text-align: right;">37253956</td>
<td style="text-align: right;">1257</td>
</tr>
<tr class="even">
<td style="text-align: left;">Colorado</td>
<td style="text-align: left;">CO</td>
<td style="text-align: left;">West</td>
<td style="text-align: right;">5029196</td>
<td style="text-align: right;">65</td>
</tr>
</tbody>
</table>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>ggplot1 <span class="ot">=</span> murders <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="at">mapping =</span> <span class="fu">aes</span>(<span class="at">x=</span>population<span class="sc">/</span><span class="dv">10</span><span class="sc">^</span><span class="dv">6</span>, <span class="at">y=</span>total)) </span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>  ggplot1 <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">col=</span>region), <span class="at">size =</span> <span class="dv">4</span>) <span class="sc">+</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_text_repel</span>(<span class="fu">aes</span>(<span class="at">label=</span>abb)) <span class="sc">+</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_log10</span>() <span class="sc">+</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_log10</span>() <span class="sc">+</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">formula =</span> <span class="st">"y~x"</span>, <span class="at">method=</span>lm,<span class="at">se =</span> F)<span class="sc">+</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">"Populations in millions (log10 scale)"</span>) <span class="sc">+</span> </span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">"Total number of murders (log10 scale)"</span>) <span class="sc">+</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"US Gun Murders in 2010"</span>) <span class="sc">+</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_discrete</span>(<span class="at">name =</span> <span class="st">"Region"</span>)<span class="sc">+</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>      <span class="fu">theme_wsj</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="index_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</section>
<section id="statistical-modeling" class="level3">
<h3 class="anchored" data-anchor-id="statistical-modeling">Statistical Modeling</h3>
</section>
<section id="conlusion" class="level3">
<h3 class="anchored" data-anchor-id="conlusion">Conlusion</h3>
</section>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<p>#Rough Draft of References and their summaries: Xiong, Chen, S., Tang, B., Chen, Q., Wang, X., Yan, J., &amp; Zhou, Y. (2021). Improving deep learning method for biomedical named entity recognition by using entity definition information. BMC Bioinformatics, 22(Suppl 1), 600–600. https://doi-org.ezproxy.lib.uwf.edu/10.1186/s12859-021-04236-y</p>
<p>-This article focuses on biomedical named entity recognition. This is used for text mining of biomedical texts to find different entity mentions and determine their type. This article investigated how to use entity definition for squad-style machine reading comprehension and span-level one-pass methods. This article defined Biomedical named entity recognition as “a fundamental task of biomedical text mining to identify biomedical entity mentions of different types in biomedical text.” Meaning that Biomedical NER is used to comb through biomedical texts to identify different mentions and categorize them (from my small understanding of this topic). The article even mentions that the entity type meanings can be represented by its definition, citing an example of the definition of PROTEINAS. This paper wanted to be able to encode these definitions into two different methods, mentioned above. In doing this, the article explains that introducing entity definition information is effective and improved both method types meaning it is useful for deep learning with biomedical NER. They credit this with comparisions to previous models saying that their model can recognize more due to it having more domain knowledge in with the definition information.</p>
<p>Jian Liu, Lei Gao, Sujie Guo, Rui Ding, Xin Huang, Long Ye, Qinghua Meng, Asef Nazari, Dhananjay Thiruvady, A hybrid deep-learning approach for complex biochemical named entity recognition, Knowledge-Based Systems, Volume 221, 2021, 106958, ISSN 0950-7051, https://doi.org/10.1016/j.knosys.2021.106958. (https://www.sciencedirect.com/science/article/pii/S0950705121002215)</p>
<ul>
<li>The article immediately states, “NER provides support for text mining in biochemical reactions, including entity relation extraction, attribute extraction, and metabolic response relationship extraction.” Showing that NER is used across biochemestry for a variety of reasons and information extraction. The authors go on to define and explain that NER refers to the identification of entities that have a specific meaning in the text. They detail where it was first introduced and detail how it is an important tool in information extraction. Much like the other articles I have read, this one also stated that rule based methods are limited in their ability to carry out recognition tasks since they rely on their rules. This article explains that deep learning methods are becoming more mainstream since the addres some of the shortcomings of supervised machine learning methods and can also alleviate issues that other models might have. This article does state that to train deep learning models, it ususally takes a large number of manually labelled data which can be very hard to do in a majority of fields. This article detailed more about the specifics of deep learning methods for NER which was helpful to read about since this is a fairly new topic for me.</li>
</ul>
<p>https://arxiv.org/pdf/1706.03762.pdf</p>
<p>https://arxiv.org/pdf/1910.10683.pdf</p>
<p>https://arxiv.org/pdf/1810.04805.pdf</p>
<p>https://arxiv.org/abs/1910.11470</p>
<p>https://ojs.aaai.org/index.php/AAAI/article/view/3861</p>
<p>https://arxiv.org/abs/1909.10649</p>
<p>Devlin, J. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. ArXiv.Org. https://doi.org/10.48550/arXiv.1810.04805</p>
<p>Vaswani, A. (2017). Attention Is All You Need. ArXiv.Org. https://doi.org/10.48550/arXiv.1706.03762</p>

</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-R-base" class="csl-entry" role="doc-biblioentry">
R Core Team. 2019. <em>R: A Language and Environment for Statistical Computing</em>. Vienna, Austria: R Foundation for Statistical Computing. <a href="https://www.R-project.org">https://www.R-project.org</a>.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>